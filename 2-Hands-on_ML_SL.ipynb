{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands-on Machine Learning (ML) and Sequential Learning (SL)\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/paolodeangelis/AEM/blob/main/2-Hands-on_ML_SL.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sDQxj2ccDEsW",
    "outputId": "ee5f25ba-f7c3-40b7-9d19-ac3b4b366e32"
   },
   "outputs": [],
   "source": [
    "%pip install pymatgen==2020.1.28\n",
    "%pip install matminer==0.6.2\n",
    "%pip install scikit_learn==0.22.2\n",
    "%pip install shap==0.38.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "akulIUmZEAZI",
    "outputId": "3731837b-3e53-4f87-f7d5-1b1f9220cbb7"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shap\n",
    "import sklearn\n",
    "from matminer.featurizers import composition as cf\n",
    "from matminer.featurizers.base import MultipleFeaturizer\n",
    "from pymatgen import Composition\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_selection import SelectPercentile, VarianceThreshold, f_regression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F_SCNfYSOyTM"
   },
   "outputs": [],
   "source": [
    "class MyDecorrelator(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Class to be imported in pipeline (below) for dropping the most correlated columns, preventing data leakage.\"\"\"\n",
    "\n",
    "    def __init__(self, threshold):\n",
    "        self.threshold = threshold\n",
    "        self.correlated_columns = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        correlated_features = set()\n",
    "        X = pd.DataFrame(X)\n",
    "        corr_matrix = X.corr()\n",
    "        for i in range(len(corr_matrix.columns)):\n",
    "            for j in range(i):\n",
    "                if (\n",
    "                    abs(corr_matrix.iloc[i, j]) > self.threshold\n",
    "                ):  # we are interested in absolute coeff value\n",
    "                    colname = corr_matrix.columns[i]  # getting the name of column\n",
    "                    correlated_features.add(colname)\n",
    "        print(np.shape(pd.DataFrame(X).drop(labels=correlated_features, axis=1)))\n",
    "        # print(pd.DataFrame(X).drop(labels=correlated_features, axis=1))\n",
    "        self.correlated_features = correlated_features\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None, **kwargs):\n",
    "        return (pd.DataFrame(X)).drop(labels=self.correlated_features, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fNWMtr9hNZY9"
   },
   "outputs": [],
   "source": [
    "def get_compostion(\n",
    "    c,\n",
    "):  # Function to get compositions from chemical formula using pymatgen\n",
    "    try:\n",
    "        return Composition(c)\n",
    "    except:  # noqa: E722\n",
    "        return None\n",
    "\n",
    "\n",
    "def featurizing(data, property_interest=None):\n",
    "\n",
    "    # Featurizer\n",
    "    f = MultipleFeaturizer(\n",
    "        [\n",
    "            cf.Stoichiometry(),\n",
    "            cf.ElementProperty.from_preset(\"magpie\"),\n",
    "            cf.ValenceOrbital(props=[\"avg\"]),\n",
    "            cf.IonProperty(fast=True),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Inputs\n",
    "    data[\"composition\"] = [get_compostion(mat) for mat in data.Components]\n",
    "\n",
    "    featurized_data = pd.DataFrame(\n",
    "        f.featurize_many(data[\"composition\"], ignore_errors=True),\n",
    "        columns=f.feature_labels(),\n",
    "        index=data[\"Components\"],\n",
    "    )\n",
    "    if property_interest:\n",
    "        featurized_data[property_interest] = data[property_interest].values\n",
    "    return featurized_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ktUaj09Yqqa"
   },
   "source": [
    "## Handle data\n",
    "\n",
    "\n",
    "1.   Import data (composition and target property for each material)\n",
    "2.   Extract 145 composition based features for each material\n",
    "3.   Drop rows with NaN\n",
    "4.   Split data in a training set (80% of the database) and in a testing set (20% of the database)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vubFkO3MXqSM"
   },
   "outputs": [],
   "source": [
    "data = pd.read_excel(r\"Supercon_data_clean.xlsx\")  # Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jNBNR8FzN3Et",
    "outputId": "c93d2649-d391-4475-ad78-51ec4d235ce5"
   },
   "outputs": [],
   "source": [
    "Featurized_data = featurizing(data, \"Tc\")  # Extract composition based features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 554
    },
    "id": "9RdRWBKaYylT",
    "outputId": "477b4d3f-1898-4768-93bc-a327c69fb1d8"
   },
   "outputs": [],
   "source": [
    "Featurized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YB7KV9FSZPhB"
   },
   "outputs": [],
   "source": [
    "Featurized_data = Featurized_data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 554
    },
    "id": "9gaxwDKOZTpt",
    "outputId": "8cf21cd0-0ecb-4ec3-f329-f0e2c0488a4e"
   },
   "outputs": [],
   "source": [
    "Featurized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HVzqlpjuZc9t"
   },
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(\n",
    "    Featurized_data, test_size=0.2, random_state=0\n",
    ")  # split data in training set (80% of the database) and testing set (20% of the database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 554
    },
    "id": "BmyX_OiNaAJr",
    "outputId": "3546ae82-bd0f-4799-8b1f-f4dc465a9675"
   },
   "outputs": [],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "07yLVIyrNLxY"
   },
   "source": [
    "# Train and validate the predictive model\n",
    "\n",
    "\n",
    "\n",
    "1.   Define a pipeline of actions to be performed over data\n",
    "2.   Define a grid of hyperparameters to be tuned\n",
    "3.   Perform a grid search (i.e., try all the possible combinations of hyperparameters) in 5 fold cross validation\n",
    "4.   Show the performances of the best pipeline by doing predictions over the testing set\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fekGYIP3YeE1"
   },
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor(random_state=0)\n",
    "\n",
    "pipe = Pipeline(\n",
    "    [\n",
    "        (\n",
    "            \"decorrelation\",\n",
    "            MyDecorrelator(0.9),\n",
    "        ),  # Drop features with correlation above 0.9\n",
    "        (\"threshold\", VarianceThreshold(threshold=0)),  # Drop features with no variance\n",
    "        (\n",
    "            \"feature_selector\",\n",
    "            SelectPercentile(f_regression),\n",
    "        ),  # Select features in terms of the most relevant for f_regression\n",
    "        (\"rf\", rf),  # Train Random Forest\n",
    "    ],\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TRx84hCBYgnV"
   },
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"rf__n_estimators\": [100, 200],  # Tune the number of estimators\n",
    "    \"rf__max_features\": [\n",
    "        \"auto\",\n",
    "        \"sqrt\",\n",
    "    ],  # Tune the number of features to consider when looking for the best split\n",
    "    \"feature_selector__percentile\": [\n",
    "        50,\n",
    "        100,\n",
    "    ],  # Tune the percentage of features to retain in terms of f_regression score\n",
    "}\n",
    "search = GridSearchCV(pipe, param_grid, n_jobs=-1, verbose=1, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZO3WJmSHY75X",
    "outputId": "1ec7d5cf-b7bd-41f6-d511-6778bc62a7c5"
   },
   "outputs": [],
   "source": [
    "search.fit(train_df.iloc[:, :-1], train_df.iloc[:, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bmTBnNuNZA2U",
    "outputId": "9e358569-d846-4cf4-b464-821a6764c9f6"
   },
   "outputs": [],
   "source": [
    "search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EwP5pjYPebaH"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dmKs8YPqcQ0V"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 578
    },
    "id": "LRyogcsIenWF",
    "outputId": "ef590c62-b02e-43d1-e12b-06e41fcd265e"
   },
   "outputs": [],
   "source": [
    "test_predictions = search.predict(\n",
    "    test_df.iloc[:, :-1]\n",
    ")  # Predicted y over samples of the testing set\n",
    "test_labels = test_df.iloc[:, -1].values  # True y over samples of the testing set\n",
    "\n",
    "r2 = sklearn.metrics.r2_score(\n",
    "    test_labels, test_predictions\n",
    ")  # coefficient of determination\n",
    "mae = mean_absolute_error(test_labels, test_predictions)  # mean absolute error\n",
    "rmse = np.sqrt(\n",
    "    mean_squared_error(test_labels, test_predictions)\n",
    ")  # root mean squared error\n",
    "delta = max(test_labels) - min(test_labels)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(3, 3), dpi=190)\n",
    "plt.scatter(test_labels, test_predictions, c=\"crimson\", alpha=0.2)\n",
    "p1 = max(max(test_predictions), max(test_labels))\n",
    "p2 = min(min(test_predictions), min(test_labels))\n",
    "plt.plot([p1, p2], [p1, p2], \"b-\")\n",
    "plt.annotate(\n",
    "    \"$R^2$ = %0.3f\" % r2,\n",
    "    xy=(0.02 * delta, 0.95 * delta),\n",
    "    xytext=(0.02 * delta, 0.95 * delta),\n",
    ")\n",
    "plt.annotate(\n",
    "    \"MAE = %0.3f K\" % mae,\n",
    "    xy=(0.02 * delta, 0.85 * delta),\n",
    "    xytext=(0.02 * delta, 0.85 * delta),\n",
    ")\n",
    "plt.annotate(\n",
    "    \"RMSE = %0.3f K\" % rmse,\n",
    "    xy=(0.02 * delta, 0.75 * delta),\n",
    "    xytext=(0.02 * delta, 0.75 * delta),\n",
    ")\n",
    "plt.xlabel(\"True $T_c$ (K)\")\n",
    "plt.ylabel(\"Predicted $T_c$ (K)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jk7CFU2UT8ad"
   },
   "source": [
    "# Manual refit\n",
    "We manually refit the best pipeline over the all training set, in order to isolate the Random Forest model and to compute the SHAP values with the TreeExplainer over the entire testing set. Otherwise, the pipeline would be seen as a black box by SHAP, and we would be compelled to use the agnostic KernelSHAP interpretation algorithm (which is more inefficient)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kKaE4wo3eqND"
   },
   "outputs": [],
   "source": [
    "X_train = train_df.iloc[:, :-1].loc[\n",
    "    :, VarianceThreshold(threshold=0).fit(train_df.iloc[:, :-1]).get_support()\n",
    "]  # Drop features with 0 variance\n",
    "X_test = test_df[X_train.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fIXGJRo0i-Re"
   },
   "outputs": [],
   "source": [
    "correlated_features = set()\n",
    "corr_matrix = X_train.corr()\n",
    "for i in range(len(corr_matrix.columns)):\n",
    "    for j in range(i):\n",
    "        if (\n",
    "            abs(corr_matrix.iloc[i, j]) > 0.9\n",
    "        ):  # we are interested in absolute coeff value\n",
    "            colname = corr_matrix.columns[i]  # getting the name of column\n",
    "            correlated_features.add(colname)\n",
    "\n",
    "X_train = X_train.drop(labels=correlated_features, axis=1)  # drop correlated features\n",
    "X_test = X_test[X_train.columns]\n",
    "y_train = train_df.iloc[:, -1]\n",
    "y_test = test_df.iloc[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WZgsqcsitZ-E"
   },
   "outputs": [],
   "source": [
    "X_train = X_train.loc[\n",
    "    :,\n",
    "    SelectPercentile(f_regression, percentile=100).fit(X_train, y_train).get_support(),\n",
    "]  # Select the percentile of best features from the best pipeline\n",
    "X_test = X_test[X_train.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 554
    },
    "id": "lVXsTp5rUQOu",
    "outputId": "1896d290-c1df-4dcd-975d-ab5ca6e2436b"
   },
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DCVg4cQGtnul",
    "outputId": "c2c680df-0b19-4ba2-fa30-8d6e11452fb5"
   },
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor(random_state=0, n_estimators=100, max_features=\"auto\")\n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 578
    },
    "id": "_U4xm2lrvOFg",
    "outputId": "de033406-7af8-4e06-9ff1-e12bf2b0e64a"
   },
   "outputs": [],
   "source": [
    "y_predictions = rf.predict(X_test)\n",
    "r2 = sklearn.metrics.r2_score(y_test, y_predictions)\n",
    "mae = mean_absolute_error(y_test, y_predictions)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_predictions))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(3, 3), dpi=190)\n",
    "plt.scatter(test_labels, y_predictions, c=\"crimson\", alpha=0.2)\n",
    "p1 = max(max(y_predictions), max(y_test))\n",
    "p2 = min(min(y_predictions), min(y_test))\n",
    "plt.plot([p1, p2], [p1, p2], \"b-\")\n",
    "\n",
    "plt.annotate(\n",
    "    \"$R^2$ = %0.3f\" % r2,\n",
    "    xy=(0.02 * delta, 0.95 * delta),\n",
    "    xytext=(0.02 * delta, 0.95 * delta),\n",
    ")\n",
    "plt.annotate(\n",
    "    \"MAE = %0.3f K\" % mae,\n",
    "    xy=(0.02 * delta, 0.85 * delta),\n",
    "    xytext=(0.02 * delta, 0.85 * delta),\n",
    ")\n",
    "plt.annotate(\n",
    "    \"RMSE = %0.3f K\" % rmse,\n",
    "    xy=(0.02 * delta, 0.75 * delta),\n",
    "    xytext=(0.02 * delta, 0.75 * delta),\n",
    ")\n",
    "plt.xlabel(\"True $T_c$ (K)\")\n",
    "plt.ylabel(\"Predicted $T_c$ (K)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dcc6mXGIv1zB"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kC3CNtj5e6NF"
   },
   "source": [
    "# Interpretability\n",
    "Thanks to the TreeSHAP algorithm, we can find the most relevant features, ranking them in terms of importance with respect to the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6eHapUFZ4N_o"
   },
   "outputs": [],
   "source": [
    "explainer = shap.TreeExplainer(rf)\n",
    "shap_values = explainer.shap_values(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4GmMMF644RDx"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 566
    },
    "id": "bvfDU5LZ4WmU",
    "outputId": "f0164f78-6daa-490c-c7be-e1cfc8354739"
   },
   "outputs": [],
   "source": [
    "N = np.shape(X_test)[1]\n",
    "k = 0.75\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cumsum = np.cumsum(np.sort(np.mean(abs(shap_values), axis=0))[::-1])\n",
    "normalized_cumulative = np.cumsum(np.sort(np.mean(abs(shap_values), axis=0))[::-1]) / (\n",
    "    np.max(np.cumsum(np.sort(np.mean(abs(shap_values), axis=0))[::-1]))\n",
    ")\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(3, 3), dpi=190)\n",
    "ax.plot(np.arange(N), normalized_cumulative)\n",
    "ax.plot(np.arange(N), k * np.ones(N))\n",
    "ind_cross1 = np.argmin(\n",
    "    np.fabs(normalized_cumulative - k * max(normalized_cumulative) * np.ones(N))\n",
    ")\n",
    "# plt.yticks(np.array([0, 0.5, 1]))\n",
    "\n",
    "ax.annotate(\n",
    "    \"%i features\" % (ind_cross1 + 1),\n",
    "    xy=(ind_cross1 + 1, 0.01),\n",
    "    xytext=(ind_cross1 + 10, 0.2),\n",
    "    arrowprops=dict(facecolor=\"black\", shrink=0.000005, width=0.1, headwidth=4),\n",
    ")\n",
    "ax.annotate(\n",
    "    \"75% of\\nthe maximum\",\n",
    "    xy=(90, 0.73),\n",
    "    xytext=(100, 0.45),\n",
    "    arrowprops=dict(facecolor=\"black\", shrink=0.0005, width=0.1, headwidth=4),\n",
    ")\n",
    "plt.scatter(ind_cross1, normalized_cumulative[ind_cross1], color=\"orange\")\n",
    "plt.plot(\n",
    "    (ind_cross1, ind_cross1),\n",
    "    (normalized_cumulative[ind_cross1], 0),\n",
    "    color=\"orange\",\n",
    "    ls=\":\",\n",
    ")\n",
    "plt.ylim(0, 1.04)\n",
    "plt.xlabel(\"Features\")\n",
    "plt.ylabel(\"Normalized\\ncumulative importance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 477
    },
    "id": "O8CB89mepBe7",
    "outputId": "75bae6bf-d15c-40f0-b0ed-98de68bae851"
   },
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values, X_test, max_display=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-tkdw3UXpN5z"
   },
   "outputs": [],
   "source": [
    "Output_shap = pd.DataFrame(shap_values, index=X_test.index, columns=X_test.columns)\n",
    "Output_mean_shap = pd.DataFrame(\n",
    "    abs(Output_shap).describe().loc[\"mean\"]\n",
    "    / sum(abs(Output_shap).describe().loc[\"mean\"])\n",
    ").sort_values(\"mean\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1tj_3qyWpe94"
   },
   "outputs": [],
   "source": [
    "Output_mean_shap.to_excel(\n",
    "    \"Output_mean_shap.xlsx\"\n",
    ")  # list of the features ranked in terms of importance (importances sum up to 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oaVG9R3iliWB"
   },
   "source": [
    "# Sequential Learning\n",
    "We consider a random subset of 100 rows of the original database; we perform SL starting with the worst 50 materials in this set, in terms of the target property.\n",
    "\n",
    "With these very same initial conditions, we perform three parallel SL procedures, choosing the next interesting material to be tested with three different acquisition functions: Maximum Expected Improvement (MEI), Maximum Likelihood Improvement (MLI), Maximum Uncertainty (MU).\n",
    "\n",
    "Since the random forest regressor by lolopy (used to compute $\\mu$ and $\\sigma$ for each of the materials of the testing set) is not deterministic, at each iteration of SL, given an acquisition function, we repeat the regression, and the consequent choice of the next material, 10 times; we effectively pick the most preferred material. We repeat the procedure for the three acquisition functions in parallel.\n",
    "\n",
    "We repeat the procedure both with only the relevant features, and with an extended set of features including also non relevant ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b-T7HP4stYY4",
    "outputId": "e11c6a29-176d-46fd-89dc-dfa20263cbdd"
   },
   "outputs": [],
   "source": [
    "%pip install lolopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9viwWUszlkE_"
   },
   "outputs": [],
   "source": [
    "from lolopy.learners import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KTrgvCwAlsE8"
   },
   "outputs": [],
   "source": [
    "def MEI(X: np.ndarray, y: np.ndarray, n_steps: int, T: int) -> int:\n",
    "    \"\"\"Acquisition functions MEI.\n",
    "\n",
    "    Args:\n",
    "        X (numpy.ndarray): matrix with n rows (number of total materials for\n",
    "            which doing the SL, in our case 100) and d columns (number of features\n",
    "            taken into account for the optimization)\n",
    "        y (\n",
    "\n",
    "            .ndarray): vector with n rows (target property)\n",
    "        n_steps (int): number of steps allowed for doing SL (in our case,\n",
    "            100 total materials - 50 materials in the initial training set\n",
    "            = maximum 50 steps to find the optimum)\n",
    "        T (int): number of times the (non-deterministic) regression and\n",
    "            the consequent choice of the next material is performed\n",
    "\n",
    "    Returns:\n",
    "        int: the index of the chosen material\n",
    "    \"\"\"\n",
    "\n",
    "    arr = y\n",
    "    minima = arr.argsort()[0:50]\n",
    "    in_train = np.zeros(len(X), dtype=np.bool)\n",
    "    in_train[minima] = True\n",
    "\n",
    "    all_inds = set(range(len(y)))\n",
    "    F = np.zeros(n_steps)\n",
    "    G = np.zeros(n_steps)\n",
    "    mei_train = [list(set(np.where(in_train)[0].tolist()))]\n",
    "    mei_train_inds = []\n",
    "\n",
    "    for i in tqdm.tqdm(range(n_steps)):\n",
    "        mei_train_inds = mei_train[-1].copy()\n",
    "        mei_search_inds = list(all_inds.difference(mei_train_inds))\n",
    "\n",
    "        mei_selection_index = []\n",
    "        for j in range(T):\n",
    "            model.fit(X[mei_train_inds], y[mei_train_inds])\n",
    "            mei_y_pred_prov = model.predict(X[mei_search_inds])\n",
    "            mei_selection_index.append(np.argmax(mei_y_pred_prov))\n",
    "\n",
    "        mei_index_G = max(set(mei_selection_index), key=mei_selection_index.count)\n",
    "        mei_index = mei_search_inds[mei_index_G]  # Pick the most preferred entry\n",
    "        mei_train_inds.append(mei_search_inds[mei_index_G])\n",
    "        mei_train.append(mei_train_inds)\n",
    "        G[i] = mei_index\n",
    "        F[i] = mei_train_inds[-1]\n",
    "        if mei_train_inds[-1] == np.argmax(y):\n",
    "            break\n",
    "\n",
    "    return F\n",
    "\n",
    "\n",
    "def MLI(X: np.ndarray, y: np.ndarray, n_steps: int, T: int) -> int:\n",
    "    \"\"\"Acquisition functions MLI.\n",
    "\n",
    "    Args:\n",
    "        X (numpy.ndarray): matrix with n rows (number of total materials for\n",
    "            which doing the SL, in our case 100) and d columns (number of features\n",
    "            taken into account for the optimization)\n",
    "        y (numpy.ndarray): vector with n rows (target property)\n",
    "        n_steps (int): number of steps allowed for doing SL (in our case,\n",
    "            100 total materials - 50 materials in the initial training set\n",
    "            = maximum 50 steps to find the optimum)\n",
    "        T (int): number of times the (non-deterministic) regression and\n",
    "            the consequent choice of the next material is performed\n",
    "\n",
    "    Returns:\n",
    "        int: the index of the chosen material\n",
    "    \"\"\"\n",
    "    arr = y\n",
    "    minima = arr.argsort()[0:50]\n",
    "    in_train = np.zeros(len(X), dtype=np.bool)\n",
    "    in_train[minima] = True\n",
    "\n",
    "    all_inds = set(range(len(y)))\n",
    "    K = np.zeros(n_steps)\n",
    "    L = np.zeros(n_steps)\n",
    "    mli_train = [list(set(np.where(in_train)[0].tolist()))]\n",
    "    mli_train_inds = []\n",
    "\n",
    "    for i in tqdm.tqdm(range(n_steps)):\n",
    "        mli_train_inds = mli_train[-1].copy()\n",
    "        mli_search_inds = list(all_inds.difference(mli_train_inds))\n",
    "\n",
    "        mli_selection_index = []\n",
    "        for j in range(T):\n",
    "            model.fit(X[mli_train_inds], y[mli_train_inds])\n",
    "            mli_y_pred_prov, mli_y_std_prov = model.predict(\n",
    "                X[mli_search_inds], return_std=True\n",
    "            )\n",
    "            mli_selection_index.append(\n",
    "                np.argmax(\n",
    "                    np.divide(\n",
    "                        mli_y_pred_prov - np.max(y[mli_train_inds]), mli_y_std_prov\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "\n",
    "        mli_index_L = max(set(mli_selection_index), key=mli_selection_index.count)\n",
    "        mli_index = mli_search_inds[mli_index_L]  # Pick the most preferred entry\n",
    "\n",
    "        mli_train_inds.append(mli_search_inds[mli_index_L])\n",
    "        mli_train.append(mli_train_inds)\n",
    "        L[i] = mli_index\n",
    "        K[i] = mli_train_inds[-1]\n",
    "        if mli_train_inds[-1] == np.argmax(y):\n",
    "            break\n",
    "\n",
    "    return K\n",
    "\n",
    "\n",
    "def MU(X: np.ndarray, y: np.ndarray, n_steps: int, T: int) -> int:\n",
    "    \"\"\"Acquisition functions MU.\n",
    "\n",
    "    Args:\n",
    "        X (numpy.ndarray): matrix with n rows (number of total materials for\n",
    "            which doing the SL, in our case 100) and d columns (number of features\n",
    "            taken into account for the optimization)\n",
    "        y (numpy.ndarray): vector with n rows (target property)\n",
    "        n_steps (int): number of steps allowed for doing SL (in our case,\n",
    "            100 total materials - 50 materials in the initial training set\n",
    "            = maximum 50 steps to find the optimum)\n",
    "        T (int): number of times the (non-deterministic) regression and\n",
    "            the consequent choice of the next material is performed\n",
    "\n",
    "    Returns:\n",
    "        int: the index of the chosen material\n",
    "    \"\"\"\n",
    "\n",
    "    arr = y\n",
    "    minima = arr.argsort()[0:50]\n",
    "    in_train = np.zeros(len(X), dtype=np.bool)\n",
    "    in_train[minima] = True\n",
    "\n",
    "    all_inds = set(range(len(y)))\n",
    "    R = np.zeros(n_steps)\n",
    "    S = np.zeros(n_steps)\n",
    "    mu_train = [list(set(np.where(in_train)[0].tolist()))]\n",
    "    mu_train_inds = []\n",
    "\n",
    "    for i in tqdm.tqdm(range(n_steps)):\n",
    "        mu_train_inds = mu_train[-1].copy()\n",
    "        mu_search_inds = list(all_inds.difference(mu_train_inds))\n",
    "\n",
    "        mu_selection_index = []\n",
    "        for j in range(T):\n",
    "            model.fit(X[mu_train_inds], y[mu_train_inds])\n",
    "            mu_y_pred_prov, mu_y_std_prov = model.predict(\n",
    "                X[mu_search_inds], return_std=True\n",
    "            )\n",
    "            mu_selection_index.append(np.argmax(mu_y_std_prov))\n",
    "\n",
    "        mu_index_R = max(set(mu_selection_index), key=mu_selection_index.count)\n",
    "        mu_index = mu_search_inds[mu_index_R]\n",
    "\n",
    "        mu_train_inds.append(mu_search_inds[mu_index_R])\n",
    "        mu_train.append(mu_train_inds)  # Pick the most preferred entry\n",
    "        R[i] = mu_index\n",
    "        S[i] = mu_train_inds[-1]\n",
    "        if mu_train_inds[-1] == np.argmax(y):\n",
    "            break\n",
    "\n",
    "    return S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TzK80Z7qmDUF"
   },
   "outputs": [],
   "source": [
    "def produce_Data_SL(\n",
    "    Data: pd.DataFrame,\n",
    "    Output_mean_shap: pd.DataFrame,\n",
    "    n_relevant: int,\n",
    "    target_property: str,\n",
    ") -> tuple:\n",
    "    \"\"\"Function to produce datasets for SL starting from the complete database (Featurized_data).\n",
    "\n",
    "    Args:\n",
    "        Data (pandas.DataFrame): complete database (Featureized_data)\n",
    "        Output_mean_shap (pandas.DataFrame): ranking of features used in terms of importance\n",
    "        n_relevant (int): number of relevant features\n",
    "        target_property (str): name of the target property in the complete database\n",
    "\n",
    "    Returns:\n",
    "        tuple: containing:\n",
    "            -  pandas.DataFrame: dataset with only the relevant features + the target property\n",
    "            -  pandas.DataFrame: dataset with relevant features + set of unrelevant features\n",
    "                (summing up to 30 columns) + the target property\n",
    "    \"\"\"\n",
    "\n",
    "    relevant_features = list(Output_mean_shap.iloc[:n_relevant].index)\n",
    "    unrelevant_features = list(\n",
    "        Output_mean_shap.sort_values(\"mean\").iloc[: int(30 - n_relevant)].index\n",
    "    )\n",
    "    all_features = relevant_features + unrelevant_features\n",
    "\n",
    "    relevant_features.append(target_property)\n",
    "    all_features.append(target_property)\n",
    "\n",
    "    Data_sampled = Data.sample(\n",
    "        n=100, random_state=0\n",
    "    )  # replace the random_state with your id number\n",
    "\n",
    "    Data_relevant_features = pd.DataFrame(Data_sampled, columns=relevant_features)\n",
    "    Data_all_features = pd.DataFrame(Data_sampled, columns=all_features)\n",
    "\n",
    "    return (Data_relevant_features, Data_all_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3YLeIiMatPkB"
   },
   "outputs": [],
   "source": [
    "model = RandomForestRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gfQy7NSK-osK"
   },
   "outputs": [],
   "source": [
    "Output_mean_shap = pd.read_excel(r\"Output_mean_shap.xlsx\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NzIVvZAAnmFR"
   },
   "outputs": [],
   "source": [
    "Data_relevant_features, Data_all_features = produce_Data_SL(\n",
    "    Featurized_data, Output_mean_shap, 15, \"Tc\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 554
    },
    "id": "ziHpEPyR8Ba4",
    "outputId": "46eb1db2-26d2-4253-b233-5b84a51fe671"
   },
   "outputs": [],
   "source": [
    "Data_all_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S08cjR3Z-2Xc"
   },
   "outputs": [],
   "source": [
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y5apC4lLrJuv"
   },
   "source": [
    "#### Sequential learning with relevant features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JXJq4izgmZyq",
    "outputId": "b9d1eddc-2e07-4adf-983d-09ccf9fc90c5"
   },
   "outputs": [],
   "source": [
    "MEI_index_relevant = MEI(\n",
    "    Data_relevant_features.iloc[:, :-1].values,\n",
    "    Data_relevant_features.iloc[:, -1].values,\n",
    "    50,\n",
    "    10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cFgd0ABN9bFV",
    "outputId": "3c5c7ba0-d277-4621-c3e3-2608f04b0180"
   },
   "outputs": [],
   "source": [
    "MLI_index_relevant = MLI(\n",
    "    Data_relevant_features.iloc[:, :-1].values,\n",
    "    Data_relevant_features.iloc[:, -1].values,\n",
    "    50,\n",
    "    10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "afPxYRfMp7Bf",
    "outputId": "8d7a4213-6321-44aa-a74f-3e7d8a9b9531"
   },
   "outputs": [],
   "source": [
    "MU_index_relevant = MU(\n",
    "    Data_relevant_features.iloc[:, :-1].values,\n",
    "    Data_relevant_features.iloc[:, -1].values,\n",
    "    50,\n",
    "    10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HK54JtfO9xxK"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fD87kEim-H2X",
    "outputId": "e8cfd27a-3aca-4add-df9f-077813e2aaff"
   },
   "outputs": [],
   "source": [
    "# What is the best tested materials after the first 5 iterations of MLI with only relevant features?\n",
    "# It is the last one of the following list\n",
    "Data_relevant_features.iloc[MLI_index_relevant[0:10]].iloc[:, -1].sort_values()\n",
    "\n",
    "# DO the same thing with all acquisition functions (MEI, MLI, MU) and for both relevant and\n",
    "# relevant+unrelevant features based datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nByjqC6FrXMu"
   },
   "source": [
    "#### Sequential learning with 100 features (relevant + (100-relevant))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7HIc1lBkroCH",
    "outputId": "fe29e771-fd01-460f-aaef-2011082ac394"
   },
   "outputs": [],
   "source": [
    "MEI_index_all = MEI(\n",
    "    Data_all_features.iloc[:, :-1].values, Data_all_features.iloc[:, -1].values, 50, 10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "526TWnoIAhxd",
    "outputId": "9881e230-cda1-4a4e-e678-aeb352cfc0a2"
   },
   "outputs": [],
   "source": [
    "MEI_index_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e6VD0JXoBkIY"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Nw4rY4-RruOc",
    "outputId": "06bb04b2-fba2-4219-bbc0-66b98f3cafe6"
   },
   "outputs": [],
   "source": [
    "MLI_index_all = MLI(\n",
    "    Data_all_features.iloc[:, :-1].values, Data_all_features.iloc[:, -1].values, 50, 10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YgawEGMkruR1",
    "outputId": "3b5d2e41-f30a-4e61-ee69-80b198dd36f2"
   },
   "outputs": [],
   "source": [
    "MU_index_all = MU(\n",
    "    Data_all_features.iloc[:, :-1].values, Data_all_features.iloc[:, -1].values, 50, 10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hBahrbRFs6Yd",
    "outputId": "529d157a-3874-47ba-9018-9fbef8c40c5e"
   },
   "outputs": [],
   "source": [
    "MU_index_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 568
    },
    "id": "sK8fJsUx4AtK",
    "outputId": "4d3ac4c9-2f57-407c-e2fc-ac45883467db"
   },
   "outputs": [],
   "source": [
    "N = 50 / 2  # number of evaluations in random choice\n",
    "labels = [\"MEI\", \"MLI\", \"MU\"]\n",
    "relevant = [\n",
    "    19 / N,\n",
    "    12 / N,\n",
    "    8 / N,\n",
    "]  # replace numbers with the numbers of evaluations performed by SL for MEI, MLI, MU with only relevant features\n",
    "all = [\n",
    "    15 / N,\n",
    "    12 / N,\n",
    "    31 / N,\n",
    "]  # replace numbers with the numbers of evaluations performed by SL for MEI, MLI, MU with also unrelevant features\n",
    "\n",
    "x = np.arange(len(labels))  # the label locations\n",
    "width = 0.2  # the width of the bars\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 3), dpi=190)\n",
    "\n",
    "rects1 = ax.bar(x - width / 2, relevant, width, label=\"15 features\")\n",
    "rects2 = ax.bar(x + width / 2, all, width, label=\"30 features\")\n",
    "plt.axhline(y=1, color=\"k\", linewidth=1, linestyle=\"--\")\n",
    "\n",
    "\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "ax.set_ylabel(\"Number of experiments/\\n number of random experiments\")\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(labels)\n",
    "ax.legend()\n",
    "\n",
    "ax.annotate(\n",
    "    \"random choice\",\n",
    "    xy=(1, 1),\n",
    "    xytext=(1.3, 1.2),\n",
    "    arrowprops=dict(facecolor=\"black\", width=0.1, headwidth=4),\n",
    ")\n",
    "\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P1y-YiJdO7JS"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "AEM.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
